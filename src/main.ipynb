{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8179d327",
   "metadata": {},
   "source": [
    "# <span style='color:Tomato;'>Load Env Variables</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3c5816f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Directory: /DATA/Ali_Data/GraphRAG-Neo4j-VMD-NAMD\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "import utils\n",
    "\n",
    "# Add the modules directory to the Python path if needed\n",
    "# sys.path.append(os.path.abspath(\"./modules\"))\n",
    "\n",
    "# load variables into env\n",
    "root_dir = utils.get_project_root()\n",
    "f = root_dir / \".secrets\" / \".env\"\n",
    "assert f.exists(), f\"File not found: {f}\"\n",
    "dotenv.load_dotenv(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61b8b91",
   "metadata": {},
   "source": [
    "# <span style='color:Tomato;'>Process PDFs</span>\n",
    "\n",
    "We'll use Langchain `PyMuPDF4LLM` to load the PDF files into LangChain documents.\n",
    "\n",
    "We'll also use LLM to convert images into a summery and extract its data.\n",
    "\n",
    "\n",
    "## <span style='color:Orange;'>Basic Imports</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e74d0d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures as cf\n",
    "import pickle\n",
    "import pprint\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "temp_dir = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf5e7cb",
   "metadata": {},
   "source": [
    "## <span style='color:Orange;'>Loading PDF file as LangChain Document</span>\n",
    "\n",
    "> Images will be extracted (to text) using a Multimodal LLM.\n",
    "\n",
    "You can either use `load()` method to do it all at once in memory or inclemently do it using `lazy_load()`.\n",
    "\n",
    "Since our docs are big, we'll use `lazy_load()` to also see the progress.\n",
    "\n",
    "To save time, we will load the docs from a pickle file if previously processed, otherwise process them and save them as a pickle.\n",
    "\n",
    "### <span style='color:Khaki;'>Custom Splitting Mode</span>\n",
    "\n",
    "> By default, each page in the PDF is a (LangChain) Document!\n",
    "\n",
    "When loading the PDF file you can split it in two different ways:\n",
    "- By page `mode=\"page\"`\n",
    "- As a single text flow `mode=\"single\"`. In other words, the whole PDF would be **one** LangChain Document. You can specify page delimiter to have the pages in the metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c23482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_filename_len(file_path: Path) -> tuple[str, int]:\n",
    "    \"\"\"\n",
    "    Update the filename length for the given file path.\n",
    "    :param file_path: The file path to update.\n",
    "    :return: A tuple containing the updated file name and its length.\n",
    "    \"\"\"\n",
    "    file_name = file_path.stem.lower()\n",
    "    with fitz.open(file_path) as pdf_doc:\n",
    "        file_len = len(pdf_doc)\n",
    "    return file_name, file_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'biopython' not found. Fuzzy Searching ...\n",
      "file_path = /DATA/Ali_Data/GraphRAG-Neo4j-VMD-NAMD/data/pdfs/BioPython.pdf\n"
     ]
    }
   ],
   "source": [
    "# pdf file\n",
    "file_path = Path() / \"..\" / \"data\" / \"pdfs\" / \"biopython.pdf\"\n",
    "\n",
    "file_path = file_path.resolve()\n",
    "file_path = utils.fuzzy_find(file_path)\n",
    "\n",
    "file_name, file_len = update_filename_len(file_path)\n",
    "\n",
    "# create directory for pkl files\n",
    "pkl_dir = file_path.parent.parent / \"pkls\"\n",
    "pkl_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"file_path = {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "308b5274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "file_path = /DATA/Ali_Data/GraphRAG-Neo4j-VMD-NAMD/data/pdfs/BioPython.pdf\n"
     ]
    }
   ],
   "source": [
    "# if a problem occurs during the loading, use this to delete previously processed pages.\n",
    "# todo: the page numbers are reindexed to zero\n",
    "\n",
    "problematic_pages = [391, 392, 395, 428]\n",
    "range_to_keep = range(391, file_len)  # 391 to 445 (exclusive)\n",
    "\n",
    "if 0:\n",
    "    display(Markdown(\"#### <span style='color:orangered;'>Warning: Deleting Pages !!!</span>\"))\n",
    "    temp_dir = Path(tempfile.mkdtemp()) if not isinstance(temp_dir, Path) else temp_dir\n",
    "    temp_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    with fitz.open(file_path) as doc:\n",
    "        # PART I: extract deleted pages\n",
    "        if len(problematic_pages) > 0:\n",
    "            range_to_keep = list(set(range_to_keep) - set(problematic_pages))  # needed for next part\n",
    "\n",
    "            temp_doc = fitz.open()\n",
    "            for page_number in problematic_pages:\n",
    "                temp_doc.insert_pdf(doc, from_page=page_number, to_page=page_number)\n",
    "\n",
    "            extract_file_path = temp_dir / f\"{file_name}_extract.pdf\"\n",
    "            temp_doc.save(extract_file_path)\n",
    "            temp_doc.close()\n",
    "\n",
    "        # ========================================================\n",
    "        # PART II: extract pages to keep\n",
    "        doc.select(range_to_keep)\n",
    "        partial_file_path = temp_dir / f\"{file_name}_partial.pdf\"\n",
    "        doc.save(partial_file_path)\n",
    "    \n",
    "    print(f\"extract_file_path = {extract_file_path}\")\n",
    "    print(f\"partial_file_path = {partial_file_path}\")\n",
    "\n",
    "print(f\"\\nfile_path = {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7ab061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test if the correct pages are extracted\n",
    "# with fitz.open(partial_file_path) as doc:\n",
    "#     print(doc[0].get_textpage().extractText())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b22254",
   "metadata": {},
   "source": [
    "#### <span style='color:LightGreen;'>How the Asynchronous Lazy Loading Loop</span>\n",
    "\n",
    "This code demonstrates an asynchronous lazy loading pattern with a progress bar. Let me explain how it works:\n",
    "\n",
    "\n",
    "##### <span style='color:SkyBlue;'>Key Components</span>\n",
    "\n",
    "1. `alazy_load()` - An asynchronous generator that yields documents one by one\n",
    "2. `async for` - Asynchronous iteration through the generator\n",
    "3. `tqdm.tqdm()` - Progress bar visualization\n",
    "4. Batching logic to process documents in chunks of 100\n",
    "\n",
    "##### <span style='color:SkyBlue;'>How the Async Loop Works</span>\n",
    "\n",
    "```python\n",
    "async for doc in tqdm.tqdm(await loader.alazy_load()):\n",
    "    # Process each document as it becomes available\n",
    "```\n",
    "\n",
    "The `await loader.alazy_load()` returns an asynchronous iterable. The `async for` loop then:\n",
    "\n",
    "1. Asynchronously requests the next document\n",
    "2. Waits for it to be retrieved without blocking the event loop\n",
    "3. Updates the progress bar via `tqdm`\n",
    "4. Processes the document once available\n",
    "\n",
    "The batching logic (collecting 100 pages before processing) allows for more efficient operations on groups of documents rather than one at a time.\n",
    "\n",
    "This pattern is especially useful when loading documents involves network requests or other I/O operations that would otherwise block execution.\n",
    "\n",
    "\n",
    "### <span style='color:Khaki;'>LLM Prompt</span>\n",
    "\n",
    "> You are an assistant tasked with summarizing images for retrieval.\n",
    "> 1. These summaries will be embedded and used to retrieve the raw image.\n",
    ">    Give a concise summary of the image that is well optimized for retrieval\n",
    "> 2. extract all the text from the image. Do not exclude any content from the page.\n",
    "> Format answer in markdown without explanatory text and without markdown delimiter ``` at the beginning.\n",
    "\n",
    "\n",
    "### <span style='color:Khaki;'>Which LLM to use?</span>\n",
    "\n",
    "- `gemma3:4b`: **biggest,** but provide a general understanding of the images.\n",
    "- `granite3.2-vision`: **small,** and fine-tunned for data extraction from images in PDF docs.\n",
    "- `moondream`: **smallest,** but only good for overall description of the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beac619d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pfile_name='biopython' -> 445 pages\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.parsers import LLMImageBlobParser\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_pymupdf4llm import PyMuPDF4LLMLoader\n",
    "\n",
    "pfile_path = file_path\n",
    "extract_images = True\n",
    "\n",
    "pfile_name, pfile_len = update_filename_len(pfile_path)\n",
    "\n",
    "if extract_images:\n",
    "    loader = PyMuPDF4LLMLoader(\n",
    "        pfile_path,\n",
    "        mode=\"page\",\n",
    "        extract_images=True,\n",
    "        images_parser=LLMImageBlobParser(model=ChatOllama(model=\"granite3.2-vision\", max_tokens=1024)),\n",
    "    )\n",
    "else:\n",
    "    loader = PyMuPDF4LLMLoader(pfile_path, mode=\"page\")\n",
    "\n",
    "print(f\"{pfile_name=} -> {pfile_len} pages\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a28c9",
   "metadata": {},
   "source": [
    "The nice thing about `lazy_load()`, is that we can stop processing any page and skip it if a problem happen.\n",
    "\n",
    "You can also resume whenever you want or process pages with different config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44fa7a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading docs from pickle\n",
      "Loaded biopython: 445 documents\n"
     ]
    }
   ],
   "source": [
    "if (pkl_dir / f\"docs_{pfile_name}.pkl\").exists():\n",
    "    print(\"Loading docs from pickle\")\n",
    "    with open(pkl_dir / f\"docs_{pfile_name}.pkl\", \"rb\") as f:\n",
    "        docs = pickle.load(f)\n",
    "else:\n",
    "    print(f\"Loading docs from pdf. \\nThis will take some time (~{int(pfile_len / 30)} min)\")  # on average 30 pages per minute\n",
    "\n",
    "    # Option 1: loading small docs\n",
    "    # docs = loader.load()\n",
    "\n",
    "    # ---------------------------\n",
    "\n",
    "    # Option 2: Load documents asynchronously (almost 3x faster)\n",
    "    # assert not extract_images, \"Async loading not supported for image extraction\"\n",
    "    # docs = await loader.aload()\n",
    "\n",
    "    # ---------------------------\n",
    "\n",
    "    # Option 3: lazy load with progress bar\n",
    "    # # todo: make this asynchronous\n",
    "    # docs = []\n",
    "    # for doc in tqdm(loader.lazy_load(), total=pfile_len):\n",
    "    #     docs.append(doc)\n",
    "\n",
    "    # ---------------------------\n",
    "\n",
    "    # Option 4: Load with timeout\n",
    "    # todo: not working properly when timeout is reached\n",
    "    \n",
    "    timeout_seconds = 30\n",
    "    skipped_pages = []\n",
    "    docs = []\n",
    "\n",
    "    def get_next_doc(loader):\n",
    "        return next(loader)\n",
    "\n",
    "    loader_iter = iter(loader.lazy_load())\n",
    "\n",
    "    for i in tqdm(range(pfile_len), total=pfile_len):\n",
    "        with cf.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            future = executor.submit(get_next_doc, loader_iter)\n",
    "            try:\n",
    "                doc = future.result(timeout=timeout_seconds)\n",
    "                docs.append(doc)\n",
    "            except cf.TimeoutError:\n",
    "                skipped_pages.append(i)\n",
    "\n",
    "    # pickle save the docs\n",
    "    with open(pkl_dir / f\"docs_{pfile_name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(docs, f)\n",
    "\n",
    "print(f\"Loaded {pfile_name}: {len(docs)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a8e462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merging docs if partially processed\n",
    "# with open(pkl_dir / \"docs_w.img.biopython_part1.pkl\", \"rb\") as f:\n",
    "#     docs0 = pickle.load(f)\n",
    "\n",
    "# with open(pkl_dir / \"docs_w.img.biopython_partial.pkl\", \"rb\") as f:\n",
    "#     docs1 = pickle.load(f)\n",
    "\n",
    "# with open(pkl_dir / \"docs_biopython_extract.pkl\", \"rb\") as f:\n",
    "#     docs2 = pickle.load(f)\n",
    "\n",
    "# docs = docs0 + docs1 + docs2\n",
    "\n",
    "# print(f\"Loaded {len(docs)} documents\")\n",
    "\n",
    "# with open(pkl_dir / f\"docs_biopython.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(docs, f)\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "# # Correcting the page numbers\n",
    "# correct_pages = list(range(0, 445))\n",
    "# extract_pages = [391, 392, 395, 428]\n",
    "\n",
    "# for i in extract_pages:\n",
    "#     correct_pages.remove(i)\n",
    "# correct_pages = correct_pages + extract_pages\n",
    "\n",
    "# for d, i in zip(docs, correct_pages):\n",
    "#     d.metadata[\"page\"] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ac331c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = docs[-5]\n",
    "# display(Markdown(temp.page_content))\n",
    "# print('-'*50)\n",
    "# pprint.pp(temp.metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755813f9",
   "metadata": {},
   "source": [
    "## <span style='color:Orange;'>Graph Database</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ab52954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jGraph\n",
    "\n",
    "graph = Neo4jGraph()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff05a99",
   "metadata": {},
   "source": [
    "## <span style='color:Orange;'>Doc Chunking (Spiting)</span>\n",
    "\n",
    "We'll split text based on semantic similarity instead of character based. Inspired by the [5 Levels Of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb).\n",
    "To instantiate a [SemanticChunker](https://python.langchain.com/api_reference/experimental/text_splitter/langchain_experimental.text_splitter.SemanticChunker.html), we must specify an embedding model first.\n",
    "\n",
    "```python\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "text_splitter = SemanticChunker(EmbeddingModel())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132709aa",
   "metadata": {},
   "source": [
    "### <span style='color:Khaki;'>Custom `EmbeddingModel` Class</span>\n",
    "\n",
    "This class must conform to the `Embeddings` interface because `SemanticChunker` expects an object that implements the `embed_documents` (or `embed_query`) methods.\n",
    "Just a standalone function `embed_content()` wonâ€™t satisfy the interface that `SemanticChunker` relies on.\n",
    "\n",
    "```python\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "class CustomEmbeddingModel(Embeddings):\n",
    "    def __init__(self, task_type=\"SEMANTIC_SIMILARITY\"):\n",
    "        self.task_type = task_type\n",
    "\n",
    "    @retry.Retry(predicate=is_retriable)  # if you have a Retry function (like Gemini)\n",
    "    def embed_documents(self, input: Documents) -> Embeddings:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (Documents: list[str])\n",
    "\n",
    "        Returns:\n",
    "            Embeddings (list[list[float]])\n",
    "        \"\"\"\n",
    "        response = client.models.embed_content(\n",
    "            model=\"models/text-embedding-004\", contents=input, config=types.EmbedContentConfig(task_type=self.task_type)\n",
    "        )\n",
    "        return [e.values for e in response.embeddings]\n",
    "    \n",
    "    def embed_query():\n",
    "        print(\"embed_query() not implemented\")\n",
    "        return None\n",
    "\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        return self.embed_documents(input)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2192845",
   "metadata": {},
   "source": [
    "\n",
    "### <span style='color:Khaki;'>Gemmini Embeddings</span>\n",
    "\n",
    "If you use your free Google quota for embedding, it will exhaust it.\n",
    "We'll use an open source embedding for text splitting.\n",
    "\n",
    "\n",
    "```python\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.api_core import retry\n",
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "print(genai.__version__)\n",
    "\n",
    "# the api is loaded from the env\n",
    "client = genai.Client()\n",
    "\n",
    "for m in client.models.list():\n",
    "    if \"embedContent\" in m.supported_actions:\n",
    "        print(m.name)\n",
    "\n",
    "# Define a helper to retry when per-minute quota is reached.\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "Embedding_Model = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", task_type=\"SEMANTIC_SIMILARITY\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf64ff",
   "metadata": {},
   "source": [
    "### <span style='color:Khaki;'>Create Text Splitter</span>\n",
    "\n",
    "**What is the difference between `transform_documents(documents: Sequence[Document])` and `split_documents(documents: Iterable[Document])`?**\n",
    "\n",
    "`transform_documents()` is just a wrapper around `split_documents()`. So both end up producing the same split documents.\n",
    "If you already have a list of documents and just want them split, you can call `split_documents()` directly.\n",
    "If your pipeline expects a `transform_documents()` method (as defined by the `BaseDocumentTransformer` interface), use `transform_documents()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cdf8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading split docs from pickle\n",
      "Loaded biopython: 1530 documents\n"
     ]
    }
   ],
   "source": [
    "from my_langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "pkl_name = pkl_dir / f\"docs_split_{pfile_name}.pkl\"\n",
    "\n",
    "if pkl_name.exists():\n",
    "    print(\"Loading split docs from pickle\")\n",
    "    with open(pkl_name, \"rb\") as f:\n",
    "        docs_split = pickle.load(f)\n",
    "else:\n",
    "    text_splitter = SemanticChunker(\n",
    "        embeddings=OllamaEmbeddings(model=\"bge-m3\"), add_start_index=True, show_progress=True, save_temp=True\n",
    "    )\n",
    "    docs_split = text_splitter.split_documents(docs)\n",
    "\n",
    "    with open(pkl_name, \"wb\") as f:\n",
    "        pickle.dump(docs_split, f)\n",
    "\n",
    "print(f\"Loaded {pfile_name}: {len(docs_split)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9972f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from my_langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "\n",
    "pkl_name = pkl_dir / f\"docs_graph_{pfile_name}.pkl\"\n",
    "\n",
    "if pkl_name.exists():\n",
    "    print(\"Loading graph docs from pickle\")\n",
    "    with open(pkl_name, \"rb\") as f:\n",
    "        graph_documents = pickle.load(f)\n",
    "else:\n",
    "    llm = OllamaFunctions(model=\"gemma3\", temperature=0, format=\"json\")\n",
    "    llm_transformer = LLMGraphTransformer(llm=llm, show_progress=True, save_temp=True)\n",
    "    graph_documents = llm_transformer.convert_to_graph_documents(docs_split)\n",
    "\n",
    "    with open(pkl_name, \"wb\") as f:\n",
    "        pickle.dump(graph_documents, f)\n",
    "\n",
    "print(f\"Loaded {pfile_name}: {len(graph_documents)} documents\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
