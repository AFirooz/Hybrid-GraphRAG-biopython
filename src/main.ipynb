{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8179d327",
   "metadata": {},
   "source": [
    "# <span style='color:Tomato;'>Load Env Variables</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3c5816f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Directory: /DATA/Ali_Data/GraphRAG-Neo4j-VMD-NAMD\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "import utils\n",
    "\n",
    "# Add the modules directory to the Python path if needed\n",
    "# sys.path.append(os.path.abspath(\"./modules\"))\n",
    "\n",
    "# load variables into env\n",
    "root_dir = utils.get_project_root()\n",
    "f = root_dir / \".secrets\" / \".env\"\n",
    "assert f.exists(), f\"File not found: {f}\"\n",
    "dotenv.load_dotenv(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61b8b91",
   "metadata": {},
   "source": [
    "# <span style='color:Tomato;'>Process PDFs</span>\n",
    "\n",
    "We'll use Langchain `PyMuPDF4LLM` to load the PDF files into LangChain documents.\n",
    "\n",
    "We'll also use LLM to convert images into a summery and extract its data.\n",
    "\n",
    "\n",
    "## <span style='color:Orange;'>Basic Imports</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e74d0d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from copy import copy\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from time import localtime, strftime\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf5e7cb",
   "metadata": {},
   "source": [
    "## <span style='color:Orange;'>Loading PDF file as LangChain Document</span>\n",
    "\n",
    "> Images will be extracted (to text) using a Multimodal LLM.\n",
    "\n",
    "You can either use `load()` method to do it all at once in memory or inclemently do it using `lazy_load()`.\n",
    "\n",
    "Since our docs are big, we'll use `lazy_load()` to also see the progress.\n",
    "\n",
    "To save time, we will load the docs from a pickle file if previously processed, otherwise process them and save them as a pickle.\n",
    "\n",
    "### <span style='color:Khaki;'>Custom Splitting Mode</span>\n",
    "\n",
    "> By default, each page in the PDF is a (LangChain) Document!\n",
    "\n",
    "When loading the PDF file you can split it in two different ways:\n",
    "- By page `mode=\"page\"`\n",
    "- As a single text flow `mode=\"single\"`. In other words, the whole PDF would be **one** LangChain Document. You can specify page delimiter to have the pages in the metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c23482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_filename_len(file_path: Path, page_delimiter=None) -> tuple[str, int]:\n",
    "    \"\"\"\n",
    "    Update the filename length for the given file path.\n",
    "    :param file_path: The file path to update.\n",
    "    :return: A tuple containing the updated file name and its length.\n",
    "    \"\"\"\n",
    "    file_name = file_path.stem.lower()\n",
    "    with fitz.open(file_path) as pdf_doc:\n",
    "        if page_delimiter is not None:\n",
    "            # use regex to look for the number of page delimiter in the file contents\n",
    "            file_len = len([page for page in pdf_doc if page_delimiter in page.get_text(\"text\").lower()])\n",
    "        else:\n",
    "            file_len = len(pdf_doc)\n",
    "    return file_name, file_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03202df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'biopython' not found. Fuzzy Searching ...\n",
      "file_path = /DATA/Ali_Data/GraphRAG-Neo4j-VMD-NAMD/data/pdfs/BioPython.pdf\n"
     ]
    }
   ],
   "source": [
    "# pdf file\n",
    "file_path = Path() / \"..\" / \"data\" / \"pdfs\" / \"biopython.pdf\"\n",
    "\n",
    "file_path = file_path.resolve()\n",
    "file_path = utils.fuzzy_find(file_path)\n",
    "\n",
    "file_name, file_len = update_filename_len(file_path)\n",
    "\n",
    "# create directory for pkl files\n",
    "pkl_dir = file_path.parent.parent / \"pkls\"\n",
    "pkl_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"file_path = {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "308b5274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "file_path = /DATA/Ali_Data/GraphRAG-Neo4j-VMD-NAMD/data/pdfs/BioPython.pdf\n"
     ]
    }
   ],
   "source": [
    "# if a problem occurs during the loading, use this to delete previously processed pages.\n",
    "# todo: the page numbers are reindexed to zero\n",
    "\n",
    "problematic_pages = []\n",
    "range_to_keep = range(0, 55)  # 391 to 445 (exclusive)\n",
    "\n",
    "if False:\n",
    "    import tempfile\n",
    "\n",
    "    temp_dir = Path(tempfile.mkdtemp())\n",
    "    display(Markdown(\"#### <span style='color:orangered;'>Warning: Deleting Pages !!!</span>\"))\n",
    "    temp_dir = Path(tempfile.mkdtemp()) if not isinstance(temp_dir, Path) else temp_dir\n",
    "    temp_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    with fitz.open(file_path) as doc:\n",
    "        # PART I: extract deleted pages\n",
    "        if len(problematic_pages) > 0:\n",
    "            range_to_keep = list(set(range_to_keep) - set(problematic_pages))  # needed for next part\n",
    "\n",
    "            temp_doc = fitz.open()\n",
    "            for page_number in problematic_pages:\n",
    "                temp_doc.insert_pdf(doc, from_page=page_number, to_page=page_number)\n",
    "\n",
    "            extract_file_path = temp_dir / f\"{file_name}_extract.pdf\"\n",
    "            temp_doc.save(extract_file_path)\n",
    "            temp_doc.close()\n",
    "            \n",
    "            print(f\"extract_file_path = {extract_file_path}\")\n",
    "\n",
    "        # ========================================================\n",
    "        # PART II: extract pages to keep\n",
    "        doc.select(range_to_keep)\n",
    "        partial_file_path = temp_dir / f\"{file_name}_partial.pdf\"\n",
    "        doc.save(partial_file_path)\n",
    "    \n",
    "    print(f\"partial_file_path = {partial_file_path}\")\n",
    "\n",
    "print(f\"\\nfile_path = {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7ab061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test if the correct pages are extracted\n",
    "# with fitz.open(partial_file_path) as doc:\n",
    "#     print(doc[0].get_textpage().extractText())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b22254",
   "metadata": {},
   "source": [
    "#### <span style='color:LightGreen;'>How the Asynchronous Lazy Loading Loop</span>\n",
    "\n",
    "This code demonstrates an asynchronous lazy loading pattern with a progress bar. Let me explain how it works:\n",
    "\n",
    "\n",
    "##### <span style='color:SkyBlue;'>Key Components</span>\n",
    "\n",
    "1. `alazy_load()` - An asynchronous generator that yields documents one by one\n",
    "2. `async for` - Asynchronous iteration through the generator\n",
    "3. `tqdm.tqdm()` - Progress bar visualization\n",
    "4. Batching logic to process documents in chunks of 100\n",
    "\n",
    "##### <span style='color:SkyBlue;'>How the Async Loop Works</span>\n",
    "\n",
    "```python\n",
    "async for doc in tqdm.tqdm(await loader.alazy_load()):\n",
    "    # Process each document as it becomes available\n",
    "```\n",
    "\n",
    "The `await loader.alazy_load()` returns an asynchronous iterable. The `async for` loop then:\n",
    "\n",
    "1. Asynchronously requests the next document\n",
    "2. Waits for it to be retrieved without blocking the event loop\n",
    "3. Updates the progress bar via `tqdm`\n",
    "4. Processes the document once available\n",
    "\n",
    "The batching logic (collecting 100 pages before processing) allows for more efficient operations on groups of documents rather than one at a time.\n",
    "\n",
    "This pattern is especially useful when loading documents involves network requests or other I/O operations that would otherwise block execution.\n",
    "\n",
    "\n",
    "### <span style='color:Khaki;'>LLM Prompt</span>\n",
    "\n",
    "> You are an assistant tasked with summarizing images for retrieval.\n",
    "> 1. These summaries will be embedded and used to retrieve the raw image.\n",
    ">    Give a concise summary of the image that is well optimized for retrieval\n",
    "> 2. extract all the text from the image. Do not exclude any content from the page.\n",
    "> Format answer in markdown without explanatory text and without markdown delimiter ``` at the beginning.\n",
    "\n",
    "\n",
    "### <span style='color:Khaki;'>Which LLM to use?</span>\n",
    "\n",
    "- `gemma3:4b`: **biggest,** but provide a general understanding of the images.\n",
    "- `granite3.2-vision`: **small,** and fine-tunned for data extraction from images in PDF docs.\n",
    "- `moondream`: **smallest,** but only good for overall description of the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beac619d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pfile_name='biopython' -> 445 pages\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.parsers import LLMImageBlobParser\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_pymupdf4llm import PyMuPDF4LLMLoader\n",
    "\n",
    "pfile_path = file_path\n",
    "extract_images = True\n",
    "\n",
    "pfile_name, pfile_len = update_filename_len(pfile_path)\n",
    "\n",
    "if extract_images:\n",
    "    loader = PyMuPDF4LLMLoader(\n",
    "        pfile_path,\n",
    "        mode=\"page\",  # page | single\n",
    "        extract_images=True,\n",
    "        images_parser=LLMImageBlobParser(model=ChatOllama(model=\"granite3.2-vision\", max_tokens=1024)),\n",
    "    )\n",
    "else:\n",
    "    loader = PyMuPDF4LLMLoader(pfile_path, mode=\"page\")\n",
    "\n",
    "print(f\"{pfile_name=} -> {pfile_len} pages\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a28c9",
   "metadata": {},
   "source": [
    "The nice thing about `lazy_load()`, is that we can stop processing any page and skip it if a problem happen.\n",
    "\n",
    "You can also resume whenever you want or process pages with different config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44fa7a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading docs from pickle\n",
      "Loaded biopython: 445 documents\n"
     ]
    }
   ],
   "source": [
    "pkl_name = pkl_dir / f\"docs_pages_{pfile_name}.pkl\"\n",
    "\n",
    "if pkl_name.exists():\n",
    "    print(\"Loading docs from pickle\")\n",
    "    with open(pkl_name, \"rb\") as f:\n",
    "        docs = pickle.load(f)\n",
    "else:\n",
    "    docs = []\n",
    "    try:\n",
    "        print(\n",
    "            f\"Loading docs from pdf. \\nThis will take some time (~{int(pfile_len / 30)} min)\"\n",
    "        )  # on average 30 pages per minute\n",
    "\n",
    "        # Option 1: loading small docs\n",
    "        # docs = loader.load()\n",
    "\n",
    "        # ---------------------------\n",
    "\n",
    "        # Option 2: Load documents asynchronously (almost 3x faster)\n",
    "        # assert not extract_images, \"Async loading not supported for image extraction\"\n",
    "        # docs = await loader.aload()\n",
    "\n",
    "        # ---------------------------\n",
    "\n",
    "        # Option 3: lazy load with progress bar\n",
    "        # # todo: make this asynchronous\n",
    "\n",
    "        for doc in tqdm(loader.lazy_load(), total=pfile_len):\n",
    "            docs.append(doc)\n",
    "\n",
    "        # pickle save the docs\n",
    "        with open(pkl_name, \"wb\") as f:\n",
    "            pickle.dump(docs, f)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{len(docs)=}\")\n",
    "        tname = pkl_name.with_suffix(f\".{strftime('%m%d.%H%M%S', localtime())}.pkl\")\n",
    "        with open(tname, \"wb\") as f:\n",
    "            pickle.dump(docs, f)\n",
    "        raise e\n",
    "\n",
    "print(f\"Loaded {pfile_name}: {len(docs)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a8e462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merging docs if partially processed\n",
    "# with open(pkl_dir / \"docs_w.img.biopython_part1.pkl\", \"rb\") as f:\n",
    "#     docs0 = pickle.load(f)\n",
    "\n",
    "# with open(pkl_dir / \"docs_w.img.biopython_partial.pkl\", \"rb\") as f:\n",
    "#     docs1 = pickle.load(f)\n",
    "\n",
    "# with open(pkl_dir / \"docs_biopython_extract.pkl\", \"rb\") as f:\n",
    "#     docs2 = pickle.load(f)\n",
    "\n",
    "# docs = docs0 + docs1 + docs2\n",
    "\n",
    "# print(f\"Loaded {len(docs)} documents\")\n",
    "\n",
    "# with open(pkl_dir / f\"docs_biopython.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(docs, f)\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "# # Correcting the page numbers\n",
    "# correct_pages = list(range(0, 445))\n",
    "# extract_pages = [391, 392, 395, 428]\n",
    "\n",
    "# for i in extract_pages:\n",
    "#     correct_pages.remove(i)\n",
    "# correct_pages = correct_pages + extract_pages\n",
    "\n",
    "# for d, i in zip(docs, correct_pages):\n",
    "#     d.metadata[\"page\"] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ac331c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = docs[-5]\n",
    "# display(Markdown(temp.page_content))\n",
    "# print('-'*50)\n",
    "# pprint.pp(temp.metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4e7061",
   "metadata": {},
   "source": [
    "### <span style='color:Khaki;'>Cleaning Docs</span>\n",
    "\n",
    "1. Clean the \"Contents\" section by:\n",
    "    - remove `. . .`\n",
    "    - add `page: ` before page numbers\n",
    "\n",
    "2. Remove the page numbers at the end of each doc `\\n\\n<num>\\n\\n`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca125ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4489de4dba5f4ebcad958a84c4f60927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/445 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New docs: 445\n"
     ]
    }
   ],
   "source": [
    "for doc in tqdm(docs, total=len(docs)):\n",
    "    txt = doc.page_content\n",
    "\n",
    "    # cleaning Content section\n",
    "    if doc.metadata[\"page\"] in range(0, 10):\n",
    "        # we use \". .\" so not to delete the end sentence dot\n",
    "        txt = txt.replace(\". .\", \"\").strip()\n",
    "\n",
    "        while txt.find(\"  \") > 0:\n",
    "            txt = txt.replace(\"  \", \" \")\n",
    "\n",
    "        txt = txt.replace(\" . \", \" \")\n",
    "        txt = re.sub(r\" (\\d+(?:\\d+)*?)\\n\", r\" page: \\1\\n\", txt)\n",
    "\n",
    "    txt = re.sub(r\"(\\n*)(\\d*)(\\n*)$\", r\"\", txt)  # remove page numbers\n",
    "\n",
    "    if txt in [\"\", \".\", \" \"]:\n",
    "        doc.metadata['empty'] = 1\n",
    "    doc.page_content = txt\n",
    "\n",
    "# delete empty docs\n",
    "docs = [doc for doc in docs if \"empty\" not in doc.metadata]\n",
    "\n",
    "print(f\"New docs: {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff05a99",
   "metadata": {},
   "source": [
    "## <span style='color:Orange;'>Doc Chunking (Spiting)</span>\n",
    "\n",
    "We'll split text based on semantic similarity instead of character based. Inspired by the [5 Levels Of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb).\n",
    "To instantiate a [SemanticChunker](https://python.langchain.com/api_reference/experimental/text_splitter/langchain_experimental.text_splitter.SemanticChunker.html), we must specify an embedding model first.\n",
    "\n",
    "```python\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "text_splitter = SemanticChunker(EmbeddingModel())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132709aa",
   "metadata": {},
   "source": [
    "### <span style='color:Khaki;'>Custom `EmbeddingModel` Class</span>\n",
    "\n",
    "This class must conform to the `Embeddings` interface because `SemanticChunker` expects an object that implements the `embed_documents` (or `embed_query`) methods.\n",
    "Just a standalone function `embed_content()` won’t satisfy the interface that `SemanticChunker` relies on.\n",
    "\n",
    "\n",
    "```python\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "class CustomEmbeddingModel(Embeddings):\n",
    "    def __init__(self, task_type=\"SEMANTIC_SIMILARITY\"):\n",
    "        self.task_type = task_type\n",
    "\n",
    "    @retry.Retry(predicate=is_retriable)  # if you have a Retry function (like Gemini)\n",
    "    def embed_documents(self, input: Documents) -> Embeddings:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (Documents: list[str])\n",
    "\n",
    "        Returns:\n",
    "            Embeddings (list[list[float]])\n",
    "        \"\"\"\n",
    "        response = client.models.embed_content(\n",
    "            model=\"models/text-embedding-004\", contents=input, config=types.EmbedContentConfig(task_type=self.task_type)\n",
    "        )\n",
    "        return [e.values for e in response.embeddings]\n",
    "    \n",
    "    def embed_query():\n",
    "        print(\"embed_query() not implemented\")\n",
    "        return None\n",
    "\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        return self.embed_documents(input)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2192845",
   "metadata": {},
   "source": [
    "\n",
    "### <span style='color:Khaki;'>Gemmini Embeddings</span>\n",
    "\n",
    "If you use your free Google quota for embedding, it will exhaust it.\n",
    "We'll use an open source embedding for text splitting.\n",
    "\n",
    "\n",
    "```python\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.api_core import retry\n",
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "print(genai.__version__)\n",
    "\n",
    "# the api is loaded from the env\n",
    "client = genai.Client()\n",
    "\n",
    "for m in client.models.list():\n",
    "    if \"embedContent\" in m.supported_actions:\n",
    "        print(m.name)\n",
    "\n",
    "# Define a helper to retry when per-minute quota is reached.\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "Embedding_Model = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", task_type=\"SEMANTIC_SIMILARITY\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf64ff",
   "metadata": {},
   "source": [
    "### <span style='color:Khaki;'>Create Text Splitter</span>\n",
    "\n",
    "**What is the difference between `transform_documents(documents: Sequence[Document])` and `split_documents(documents: Iterable[Document])`?**\n",
    "\n",
    "`transform_documents()` is just a wrapper around `split_documents()`. So both end up producing the same split documents.\n",
    "If you already have a list of documents and just want them split, you can call `split_documents()` directly.\n",
    "If your pipeline expects a `transform_documents()` method (as defined by the `BaseDocumentTransformer` interface), use `transform_documents()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7cdf8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary directory: /tmp/tmp0j8isv5u\n",
      "Loading split docs from pickle\n",
      "Loaded biopython: 906 documents\n"
     ]
    }
   ],
   "source": [
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from my_langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "pkl_name = pkl_dir / f\"docs_pages_semantic_split_bgem3_{pfile_name}.pkl\"\n",
    "\n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings=OllamaEmbeddings(model=\"bge-m3\"), add_start_index=True, show_progress=True, save_temp=True\n",
    ")\n",
    "\n",
    "# we'll setup a character text splitter to use if we get an exception when creating the graph\n",
    "# text_splitter_bkp = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=100)\n",
    "# docs_split = text_splitter_bkp.split_documents(documents=docs)\n",
    "\n",
    "\n",
    "if pkl_name.exists():\n",
    "    print(\"Loading split docs from pickle\")\n",
    "    with open(pkl_name, \"rb\") as f:\n",
    "        docs_split = pickle.load(f)\n",
    "else:\n",
    "    docs_split = text_splitter.split_documents(docs)\n",
    "\n",
    "    with open(pkl_name, \"wb\") as f:\n",
    "        pickle.dump(docs_split, f)\n",
    "\n",
    "print(f\"Loaded {pfile_name}: {len(docs_split)} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b360d",
   "metadata": {},
   "source": [
    "## <span style='color:Orange;'>Graph Database</span>\n",
    "\n",
    "### <span style='color:Khaki;'>Notes</span>\n",
    "\n",
    "1. Use a bigger model such as `gemma3:4b` to parse the information as smaller models like `smollm2` get stuck. To make it faster, use `gemma3:4b-it-qat` that is supposedly good and have lower requirements.\n",
    "\n",
    "2. Use `jupytext` to make this into a python script and run it in the background.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f518f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jGraph\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "\n",
    "\n",
    "# LOCAL Neo4j DB init\n",
    "graph = Neo4jGraph()\n",
    "\n",
    "\n",
    "# SERVER Neo4j DB init\n",
    "graph_vector_index = Neo4jVector.from_existing_graph(\n",
    "    embedding=OllamaEmbeddings(model=\"bge-m3\"),\n",
    "    search_type=\"hybrid\",  # hybrid search: combine vector similarity and keyword search\n",
    "    node_label=\"Document\",\n",
    "    text_node_properties=[\"text\"],  # property containing text content of the document nodes\n",
    "    embedding_node_property=\"embedding\",  # property containing vector embedding of the document nodes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb257662",
   "metadata": {},
   "source": [
    "### <span style='color:Khaki;'>Extract data to be suitable for a Graph DB</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd9972f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graph docs from pickle\n",
      "Loaded biopython: 906 documents\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from my_langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "\n",
    "pkl_name = pkl_dir / f\"docs_graph_bgem3_{pfile_name}.pkl\"\n",
    "\n",
    "if pkl_name.exists():\n",
    "    print(\"Loading graph docs from pickle\")\n",
    "    with open(pkl_name, \"rb\") as f:\n",
    "        gdocs = pickle.load(f)\n",
    "else:\n",
    "    llm = ChatOllama(model=\"gemma3:4b-it-qat\", temperature=0, format=\"json\")\n",
    "    llm_transformer = LLMGraphTransformer(llm=llm, show_progress=True, pkl_path=pkl_name, force_finish=True)\n",
    "    gdocs, sdocs = llm_transformer.convert_to_graph_documents(docs_split)\n",
    "\n",
    "    with open(pkl_name, \"wb\") as f:\n",
    "        pickle.dump(gdocs, f)\n",
    "    \n",
    "    if len(sdocs) > 0:\n",
    "        with open(pkl_name.with_suffix(\".skipped.id.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(sdocs, f)\n",
    "\n",
    "print(f\"Loaded {pfile_name}: {len(gdocs)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90041249",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_graph_documents(gdocs, baseEntityLabel=True, include_source=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42edd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector index\n",
    "vector_retriever = graph_vector_index.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aa3bea",
   "metadata": {},
   "source": [
    "### <span style='color:Khaki;'>Create Full-Text Index</span>\n",
    "\n",
    "This code block connects to the Neo4j database and creates a full-text index to enable efficient searching on entity IDs.\n",
    "\n",
    "1.  **Import Libraries:** Imports `GraphDatabase` from `neo4j` for database interaction and `os` to access environment variables.\n",
    "2.  **Connect to Neo4j:** Establishes a connection (driver) to the Neo4j instance using the URI and credentials stored in environment variables (`NEO4J_URI`, `NEO4J_USERNAME`, `NEO4J_PASSWORD`).\n",
    "3.  **Define Index Creation Function (`create_fulltext_index`):**\n",
    "    *   Takes a Neo4j transaction object (`tx`) as input.\n",
    "    *   Defines a Cypher query to create a full-text index named `fulltext_entity_id`.\n",
    "    *   This index applies to all nodes labeled `__Entity__` (which is the default label for entities created by `LLMGraphTransformer`).\n",
    "    *   The index is created on the `id` property of these nodes.\n",
    "    *   Executes the query using `tx.run(query)`.\n",
    "4.  **Define Index Execution Function (`create_index`):**\n",
    "    *   Opens a session with the Neo4j driver.\n",
    "    *   Uses `session.execute_write()` to run the `create_fulltext_index` function within a write transaction, ensuring atomicity.\n",
    "    *   Prints a confirmation message upon successful index creation.\n",
    "5.  **Execute and Cleanup:**\n",
    "    *   A `try...finally` block calls the `create_index()` function.\n",
    "    *   The `except Exception: pass` clause silently handles potential errors, such as the index already existing.\n",
    "    *   The `finally` block ensures that the database driver connection (`driver.close()`) is always closed, whether the index creation succeeded or failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8fb0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import os\n",
    "\n",
    "\n",
    "driver = GraphDatabase.driver(uri=os.environ[\"NEO4J_URI\"], auth=(os.environ[\"NEO4J_USERNAME\"], os.environ[\"NEO4J_PASSWORD\"]))\n",
    "\n",
    "\n",
    "def create_fulltext_index(tx):\n",
    "    query = \"\"\"\n",
    "    CREATE FULLTEXT INDEX `fulltext_entity_id` \n",
    "    FOR (n:__Entity__) \n",
    "    ON EACH [n.id];\n",
    "    \"\"\"\n",
    "    tx.run(query)\n",
    "\n",
    "\n",
    "# Function to execute the query\n",
    "def create_index():\n",
    "    with driver.session() as session:\n",
    "        session.execute_write(create_fulltext_index)\n",
    "        print(\"Fulltext index created successfully.\")\n",
    "\n",
    "\n",
    "# Call the function to create the index\n",
    "try:\n",
    "    create_index()\n",
    "except Exception:\n",
    "    pass\n",
    "finally:\n",
    "    # Close the driver connection\n",
    "    driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1918a570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=\"gemma3:4b\", temperature=0, format=\"json\")\n",
    "\n",
    "class Entities(BaseModel):\n",
    "    \"\"\"Identifying information about entities.\"\"\"\n",
    "\n",
    "    names: list[str] = Field(..., description=\"All the person, organization, or business entities that appear in the text\")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are extracting organization and person entities from the text.\"),\n",
    "        (\"human\", \"Use the given format to extract information from the following input: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "entity_chain = llm.with_structured_output(Entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0c836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_chain.invoke(\"what is BioPython?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2929f1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
    "\n",
    "\n",
    "def generate_full_text_query(input: str) -> str:\n",
    "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
    "    if not words:\n",
    "        return \"\"\n",
    "    full_text_query = \" AND \".join([f\"{word}~2\" for word in words])\n",
    "    print(f\"Generated Query: {full_text_query}\")\n",
    "    return full_text_query.strip()\n",
    "\n",
    "\n",
    "# Fulltext index query\n",
    "def graph_retriever(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Collects the neighborhood of entities mentioned\n",
    "    in the question\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    entities = entity_chain.invoke(question)\n",
    "    for entity in entities.names:\n",
    "        response = graph.query(\n",
    "            \"\"\"CALL db.index.fulltext.queryNodes('fulltext_entity_id', $query, {limit:2})\n",
    "            YIELD node,score\n",
    "            CALL {\n",
    "              WITH node\n",
    "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
    "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
    "              UNION ALL\n",
    "              WITH node\n",
    "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
    "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\n",
    "            }\n",
    "            RETURN output LIMIT 50\n",
    "            \"\"\",\n",
    "            {\"query\": entity},\n",
    "        )\n",
    "        result += \"\\n\".join([el[\"output\"] for el in response])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94fa63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph_retriever(\"what is BioPython?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e0184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_retriever(question: str):\n",
    "    graph_data = graph_retriever(question)\n",
    "    vector_data = [el.page_content for el in vector_retriever.invoke(question)]\n",
    "    final_data = f\"\"\"Graph data:\n",
    "{graph_data}\n",
    "vector data:\n",
    "{\"#Document \".join(vector_data)}\n",
    "    \"\"\"\n",
    "    return final_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86cbcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Use natural language and be concise.\n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = {\"context\": full_retriever, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad167be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(input=\"what is biopython?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
